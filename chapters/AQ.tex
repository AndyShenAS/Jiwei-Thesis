In the two chapters below, 
our focus switches from the chit-chat systems to interactive QA dialogue systems.
We will talk about how we can 
design an interactive dialogue agent, and how this agent can benefit from interacting with humans. 
The reason why 
we choose to study the issues with human-bot interactions under the context of factoid QA is because it is
easier to formalize the problem and evaluate various aspects of features and performances in a more domain-specific QA task.
But  
chit-chat 
 systems and interactive QA systems are closely related and 
 the methodology we proposed in this chapter is a general one: 
a chit-chat dialogue system can certainly benefit from the  types of interactions that will be studied in the two chapters below. 

In this chapter, we will discuss how we can train a bot to ask questions. 
Think about the following situation: 
when a student is asked a question by a teacher, but is not confident about the answer, 
they may ask for clarification or hints.
A good conversational agent should have this ability to
interact with a dialogue partner (the teacher/user).
However,  recent efforts have mostly focused
 on learning through fixed answers provided in the training set,
rather than through interactions.
In that case, when a learner encounters a confusing situation such as an unknown surface form (phrase or structure),
a semantically complicated sentence or an unknown word, the agent will either make a (usually poor) guess or will
redirect the user to other resources (e.g., a search engine, as in Siri).
Humans, in contrast, can adapt to many situations by asking questions.

In this chapter, 
we study this issue in the context of a knowledge-base based question answering task, where the dialogue agent has access to a knowledge base (KB)
and need to answer a question based on the KB. 
We first identify
 three categories of mistakes a learner can make during 
a question-answering based
dialogue scenario:\footnote{This
list is not exhaustive; for example, we do not address a failure in the dialogue generation stage.}
(1) the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question;
(2) the learner has a problem 
with reasoning, e.g., they fail to retrieve and connect 
the relevant knowledge to the question at hand;
(3) the learner lacks the knowledge necessary to answer the question in the first place
-- that is,
the knowledge sources the student has access to do not contain the needed information.

All the situations above can be potentially addressed through interaction with
the dialogue partner. % in subsequent dialogue acts.
Such interactions can be used to {\em learn to perform better in future dialogues}.
If a human student has problems understanding a teacher's question, they
 might ask the teacher to clarify the question.
If the student doesn't know where to start, they might
 ask the teacher to point out which known facts are most relevant.
If the student doesn't know the information needed at all, they might
 ask the teacher to tell them the knowledge they're missing, writing it down
 for future use.

In this work, we try to bridge the gap between how a human and
an end-to-end machine learning dialogue agent deal with these situations:
our student has to {\em learn how to learn}.
We hence design a simulator and a set of synthetic tasks in the movie question answering domain
that allow a bot to interact with a teacher to address the issues described above.
Using this framework, we explore how a bot can benefit from interaction by asking questions
 in both offline supervised settings and online reinforcement learning settings,
 as well as how to choose when to ask questions in the latter setting.
In both cases, we find that the learning system improves through interacting with users.

Finally, we validate our approach on real data where the teachers are humans using Amazon Mechanical Turk,
and observe similar results.

\section{Tasks}
\label{sec:tasks}
In this section we describe the dialogue tasks we designed.
They are tailored for the three different situations described in Section 1
that motivate the bot to ask questions:
(1) {\it Question Clarification},
 in which the bot  has problems understanding its dialogue partner's text;
  (2) {\it Knowledge Operation},
in which the bot
 needs to ask for help to perform reasoning steps over an existing knowledge base;
  and (3) {\it Knowledge Acquisition},
  in which the bot's knowledge is incomplete and needs to be filled. %, with important information missing.



For our experiments we adapt the WikiMovies dataset \citep{weston2015towards}, which consists
 of roughly 100k questions over 75k entities based on questions
with answers in the open movie dataset (OMDb).
 The training/dev/test sets respectively contain 181638 / 9702 / 9698 examples. The  accuracy metric corresponds to the percentage of times the student gives correct answers to the teacher's questions.

Each dialogue takes place between a teacher and a bot. In this
section we describe how we generate tasks using a simulator. Section \ref{sec:mturk}
 discusses how  we test similar setups with real data using Mechanical Turk.

The bot is first presented with facts from the OMDb KB.
This allows us to control the exact knowledge the bot has access to.
Then, we include several teacher-bot question-answer pairs unrelated to
the question the bot needs to answer, which we call \emph{conversation histories}.\footnote{
These history QA pairs can be viewed as distractions and are used to test the
bot's ability to separate the wheat from the chaff.
For each dialogue, we incorporate 5 extra QA pairs (10 sentences).}
%During the dialogue we include several teacher-bot question-answer pairs which can be viewed as conversational
%, they are not pertinent to the final part of the conversation but can be viewed as distractions to make the task harder
%histories\footnote{These history QA pairs can be viewed as distractions and are used to test the bot's ability to separate the wheat from the chaff. For each dialogue, we incorporate 5 extra QA pairs (10 sentences).}
%unreleated to the question the bot needs to answer.
%
In order to explore the benefits of asking clarification questions 
during a conversation, for each of the three scenarios, our 
simulator generated data for two different settings, namely, 
\emph{Question-Answering} (denoted by QA) and 
\emph{Asking-Question} (denoted by AQ).
%To simplify notations, we use {\it QA} (question-answering) to denote the setting where the bot never asks a question  and {\it AQ} (asking-question) to denote the setting where the bot both answers and asks questions.
For both {\it QA} and {\it AQ}, the bot needs to give an answer to the teacher's original question at the end.



\subsection{ Question Clarification.}


The QA-based dialogue system is interactive \cite{rieser2009does}: instead of just having to answer one single question, 
the QA-based system needs the ability to handle a diverse category of interaction-related issues such as 
 asking for question clarification \cite{stoyanchev2013modelling,stoyanchev2014towards}, adapting answers given 
a human's feedback \cite{rieser2009does}, 
self-learning when encountering new questions or concepts
\cite{purver2006clarie}, etc.
To handle these issues, 
 the system needs to 
 take proper  actions based on the current conversation state, which resembles the key issue addressed in the \textsc{state-based dialogue system}. 


As discussed in the previous sections, the bot needs to ask the teacher to clarify the question when it does not understand it. 
Actually, the task of {\it ask for clarification} has been studied in a variety of speech recognition 
scenarios
when a speech recognition system fails to understand a user's utterance, such as
use the generic dialog act REJECT and emit a rule-based ``Please repeat",
or ``I don't understand what you said" utterance, or generate more complicated targeted utterances
in different clarification senarios
 \cite{purver2004theory,nerbonne2005interrogative,stoyanchev2015localized}. 

In this setting, the bot does not understand the teacher's question. We focus on a
special situation where the bot does not understand the teacher because of typo/spelling mistakes,
 as shown in Figure \ref{AskC}.
We intentionally misspell some words in the questions such as replacing the word ``movie" with
``{\color{brown}{movvie}}" or ``star" with  ``{\color{brown}{sttar}}".\footnote{Many
reasons could lead to the bot not understanding the teacher's question, e.g.,
the teacher's question has an unknown phrase structure, rather than unknown words.
We choose to use spelling mistakes because of the ease of dataset construction.}
To make sure that the bot will have problems understanding the question, we guarantee that the bot
has never encountered the misspellings before---the misspelling-introducing mechanisms
in the training, dev, and test sets are different, so the same word will be misspelled in different ways in different sets.
We present two {\it AQ} tasks:
(i) {\it Question Paraphrase} where the student asks the teacher to use a paraphrase that
does not contain spelling mistakes to clarify the question by asking ``what do you mean?'';
and (ii) {\it Question Verification} where the student  asks the teacher
whether the original typo-bearing question corresponds to another question
without the spelling mistakes (e.g., ``Do you mean which film did Tom Hanks appear in?").
The teacher will give
feedback by giving a paraphrase of the original question without spelling mistakes
(e.g., ``I mean which film did Tom Hanks appear in") in {\it Question Paraphrase}
or positive/negative feedback in {\it Question Verification}.
Next the student will give an answer and the teacher will give positive/negative
feedback depending on whether the student's answer is correct.
Positive and negative feedback are variants of ``No, that's incorrect" or
``Yes, that's right".\footnote{In the datasets we build, there are 6 templates
for positive feedback and 6 templates for negative feedback.}
In these tasks, the bot has access to all relevant entries in the KB.

\subsection{Knowledge Operation} \label{sec:knolop}
The bot has  access to all the relevant knowledge (facts)
 but lacks the ability to perform necessary reasoning operations over them;
see Figure \ref{AskV}.
We focus on a special case where the bot will try to understand what are the relevant
facts. We explore two settings:
 {\it Ask For Relevant Knowledge} (Task 3) where the bot directly asks the teacher to point out the relevant KB fact and {\it Knowledge Verification} (Task 4)  where the bot asks whether the teacher's question is relevant to one particular KB fact. The teacher will point out the relevant KB fact in the {\it Ask For Relevant Knowledge}  setting
 or give a positive or negative response in the {\it Knowledge Verification} setting.
 Then the bot will give an answer to the teacher's original question
and the teacher will give feedback on the answer.



\begin{figure*}[!ht]
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
\cline{1-1}\cline{3-3}
 \\[-2ex]
{\bf  Task 1:  Question Paraphrase (QA)}  &&  {\bf Task 1:  Question Paraphrase (AQ):} \\
 kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} && kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} \\
kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks}, && kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks},   \\
 \underline{Robin Wright}, \underline{Gary Sinise} &&  \underline{Robin Wright}, \underline{Gary Sinise} \\
 kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis} && kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis}  \\
{\color{blue}T}/{\color{red}S }:  {Conversation History}.&&{\color{blue}T}/{\color{red}S }:  {Conversation History}.\\
{\color{blue}T : Which  {\color{brown} movvie} did Tom Hanks {\color{brown} sttar} in ?} &&{\color{blue}T : Which  {\color{brown}movvie} did Tom Hanks  {\color{brown}sttar} in ?}  \\
{\color{red}S : Larry Crowne }  &&  {\color{red}S : What do you mean ?}
% by  which  {\color{brown} movvie} did Tom Hanks {\color{brown} sttar} in ?}
 \\
{\color{blue}T : Wrong. (-)} && {\color{blue}T : I mean which film did Tom Hanks appear in.} \\
&&{\color{blue}T : Which  {\color{brown} movvie} did Tom Hanks {\color{brown} sttar} in ?}  \\
&&{\color{red}S : Forrest Gump }   \\
&& {\color{blue}T : That's correct. (+)} \\
\cline{1-1}\cline{3-3}
 \multicolumn{1}{c}{}
\vspace{-2mm} \\
\cline{1-1}\cline{3-3}
{\bf Task 2:  Question Verification (QA):} && {\bf Task 2: Question Verification (AQ)} \\
 kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} && kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} \\
kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks}, && kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks},   \\
 \underline{Robin Wright}, \underline{Gary Sinise} &&  \underline{Robin Wright}, \underline{Gary Sinise} \\
 kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis} && kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis}  \\
{\color{blue}T}/{\color{red}S }:  {Conversation History}.&&{\color{blue}T}/{\color{red}S }:  {Conversation History}.\\
{\color{blue}T : Which  {\color{brown} movvie} did Tom Hanks {\color{brown} sttar} in ?} &&{\color{blue}T : Which  {\color{brown}movvie} did Tom Hanks  {\color{brown}sttar} in ?}  \\
{\color{red}S : Forrest Gump } && {\color{red}S : Do you mean which film did Tom Hanks appear in ?}\\
{\color{blue}T : That's correct. (+)}&&{\color{blue}T : That's correct. (+)} \\
 &&{\color{blue}T : Which  {\color{brown} movvie} did Tom Hanks {\color{brown} sttar} in ?} \\
%&& {\color{red}S : Forrest Gump }  \\
%&& {\color{blue}T : That's correct. (+)} \\
&&{\color{red}S : Larry Crowne }   \\
&& {\color{blue}T : Sorry, that's not right. (-)} \\
\cline{1-1}\cline{3-3}
\end{tabular}
\caption[Sample dialogues for the {\it Question Clarification} tasks]{Sample dialogues for the two {\it Question Clarification} tasks (rows) using both the traditional {\it QA} setting
(left column) and {\it AQ} setting (right column).
In each case the same example is given for simplicity. Black text prefixed by ``kb:'' denotes KB knowledge that the student has access to.
{\color{blue}Blue} text is spoken by the teacher, denoted by {\color{blue}T}. {\color{blue}{(+)}} denotes a reward of 1
(and 0 otherwise) that the teacher assigns to the bot.
{\color{red}Red} text denotes responses or questions posed by the bot, denoted by  {\color{red}S}.
 {\color{brown}{Brown}} denotes typos deliberately introduced by the authors. For the {\it Question Verification} setting,
the student can either ask a correct (pertinent) question (as in this example) or an incorrect (irrelevant) one.
The teacher will give positive or negative feedback based on the correctness of the student's question.
In our offline superised learning experiments, the probability of asking pertinent questions and correctly answering the original
question from the teacher is set to 0.5. Finally, {\color{blue}T}/{\color{red}S }
denotes 5 pairs of questions and answers that are irrelevant to the rest of the conversation.
%{\color{yellow} JIWEI: COULD YOU PLEASE CHANGE THESE EXAMPLES OF BOT'S ANSWERS/QUESTIONS TO BE INCORRECT AS WELL? APPLY THIS TO ALL THE TASK FIGURES.}
}
\label{AskC}
\end{figure*}
\begin{figure*}[!ht]
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
 \multicolumn{1}{c}{}
\vspace{-2mm} \\
\cline{1-1}\cline{3-3}
 {\bf Task 3: Ask For Relevant Knowledge (AQ): }  && {\bf Task 4: Knowledge Verification (AQ):} \\
 kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} && kb: \underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks} \\
kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks}, && kb: \underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks},   \\
 \underline{Robin Wright}, \underline{Gary Sinise} &&  \underline{Robin Wright}, \underline{Gary Sinise} \\
 kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis} && kb: \underline{Forrest Gump} directed\textunderscore  by \underline{Robert Zemeckis}  \\

{\color{blue}T}/{\color{red}S }:  {Conversation History}.&&{\color{blue}T}/{\color{red}S }:  {Conversation History}.\\
{\color{blue}T : Which movie did Tom Hanks star in ?} &&{\color{blue}T : Which movie did Tom Hanks star in ?}  \\
 {\color{red}S :  Can you give me a hint ?}&& {\color{red}S : Does it have something to do with } \\
{\color{blue}T : It is related to the kb fact that} && ~~~~~{\color{red}\underline{Larry Crowne} {directed\textunderscore by} \underline{Tom Hanks}?}  \\
~~~~~{\color{blue}{\underline{Forrest Gump} {starred\textunderscore actors} \underline{Tom Hanks} }} &&{\color{blue}T : No. That's wrong. (-)} \\
 ~~~~~{\color{blue} \underline{Robin Wright}, \underline{Gary Sinise} }  &&{\color{blue}T : Which movie did Tom Hanks star in ?}\\
{\color{blue}T : Which movie did Tom Hanks star in ? }&&{\color{red}S : Forrest Gump } \\
{\color{red}S : Forrest Gump }  && {\color{blue}T : That's correct. (+)} \\
 {\color{blue}T : That's correct. (+)}  && \\
\cline{1-1}\cline{3-3}

\end{tabular}
\caption{Sample dialogues for {\it Knowledge Operation} tasks. }
\label{AskV}
\end{figure*}


\begin{figure*}[!ht]
\footnotesize
\label{MissE}
\begin{center}
\begin{tabular}{|l|l|l|}
 \multicolumn{3}{l}{{\bf }. }\\
\cline{1-1}\cline{3-3}
{\bf Task 5: Missing Question Entity (QA) }  &&{\bf  Task 5: Missing Question Entity (AQ)} \\
{\st{kb: Larry Crowne directed\textunderscore by Tom Hanks }}&& {\st{kb: Larry Crowne directed\textunderscore by Tom Hanks }}\\
{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks}} &&{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks, } } \\
kb: {Forrest Gump} starred\textunderscore actors {Sally Field} &&
kb: {Forrest Gump} starred\textunderscore actors {Sally Field}\\
kb: {Forrest Gump} directed\textunderscore  by {Robert Zemeckis} && kb: {Forrest Gump} directed\textunderscore  by {Robert Zemeckis}  \\
{\color{blue}T}/{\color{red}S }:  {Conversation History}. &&{\color{blue}T : Which movie did Tom Hanks star in ?}\\
{\color{blue}T : Which movie did Tom Hanks star in ?}&& {\color{red}S : I don't know. What's the answer?}\\
{\color{red}S : Forrest Gump }  && {\color{blue}T : The answer is Forrest Gump. }\\
{\color{blue}T : That's correct. (+)}&& {\color{blue}T}/{\color{red}S }:  {Conversation History}.  \\
&& {\color{blue}T : Which movie did Tom Hanks star in ?} \\
&& {\color{red}S : Forrest Gump }  \\
&& {\color{blue}T : That's correct. (+)} \\
\cline{1-1}\cline{3-3}
\end{tabular}
\end{center}
\begin{center}
\end{center}
\begin{comment}
{\bf Task 2: Missing Answer Entities from the KB}: the second KB ground truth is removed since it  contains the answer entity \underline{Forrest Gump}. \underline{Forrest Gump} is a brand new entity and has not been encountered  before. Other entities such as  \underline{Tom Hanks}, \underline{Robin Wright} and relation ``starred\textunderscore actors" might be encountered elsewhere before.
\end{comment}

\begin{center}
\begin{tabular}{|l|l|l|}
\cline{1-1}\cline{3-3}
{\bf  Task 6: Missing Answer Entity (AQ)} &&{\bf  Task 7: Missing Relation Entity (AQ)} \\
 kb: {Larry Crowne} {directed\textunderscore by} {Tom Hanks} && kb: {Larry Crowne} {directed\textunderscore by} {Tom Hanks} \\
{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks}} &&{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks, } } \\
{\st{kb: Forrest Gump starred\textunderscore actors Sally Field}}&&
{\st{kb: Forrest Gump starred\textunderscore actors Sally Field}}\\
{\st{kb: Forrest Gump directed\textunderscore  by Robert Zemeckis}} && kb: {Forrest Gump} directed\textunderscore  by {Robert Zemeckis}  \\
{\color{blue}T : Which movie did Tom Hanks star in ?} && {\color{blue}T : Which movie did Tom Hanks star in ?}\\
 {\color{red}S : I don't know. What's the answer?}&& {\color{red}S : I don't know. What's the answer?}\\
 {\color{blue}T : The answer is Forrest Gump. }\ && {\color{blue}T : The answer is Forrest Gump. }\\
 {\color{blue}T}/{\color{red}S }:  {Conversation History}.&& {\color{blue}T}/{\color{red}S }:  {Conversation History}.   \\
{\color{blue}T : Which movie did Tom Hanks star in ?}&&{\color{blue}T : Which movie did Tom Hanks star in ?}\\
{\color{red}S : Forrest Gump }  && {\color{red}S : Forrest Gump }  \\
{\color{blue}T : That's correct. (+)} &&{\color{blue}T : That's correct. (+)}  \\
\cline{1-1}\cline{3-3}
\end{tabular}
\end{center}
\begin{comment}
{\bf Task 3: Missing Relations from the KB}: The second KB ground truth is removed since it  contains relation starred\textunderscore actors (or start in). ``starred\textunderscore actors" is a brand new relation entity and is not encountered anywhere else before. All the other entities such as \underline{Forrest
 Gump}, \underline{Tom Hanks} and \underline{Robin Wright} might be encountered elsewhere before.
\end{comment}
\begin{center}
\begin{tabular}{|l|l|l|}
 \multicolumn{3}{l}{{\bf }. }\\
\cline{1-1}\cline{3-3}
{\bf  Task 8: Missing Triple (AQ)}   &&{\bf  Task 9: Missing Everything (AQ)} \\
{kb: Larry Crowne directed\textunderscore by Tom Hanks}&& {\st{kb: Larry Crowne directed\textunderscore by Tom Hanks}} \\
{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks}} &&{\st{kb: Forrest Gump starred\textunderscore actors Tom Hanks, } } \\
{kb: Forrest Gump starred\textunderscore actors Sally Field}&&
{\st{kb: Forrest Gump starred\textunderscore actors Sally Field}}\\
{kb: Forrest Gump directed\textunderscore  by Robert Zemeckis} &&{\st{kb: Forrest Gump directed\textunderscore  by Robert Zemeckis}}  \\
{\color{blue}T : Which movie did Tom Hanks star in ?} &&{\color{blue}T : Which movie did Tom Hanks star in ?}\\
 {\color{red}S : I don't know. What's the answer?}&& {\color{red}S : I don't know. What's the answer?}\\
 {\color{blue}T : The answer is Forrest Gump. }&& {\color{blue}T : The answer is Forrest Gump. }\\
  {\color{blue}T}/{\color{red}S }:  {Conversation History}. &&  {\color{blue}T}/{\color{red}S }:  {Conversation History}. \\
 {\color{blue}T : Which movie did Tom Hanks star in ?} && {\color{blue}T : Which movie did Tom Hanks star in ?} \\
{\color{red}S : Forrest Gump }  && {\color{red}S : Forrest Gump }  \\
 {\color{blue}T : That's correct. (+)} && {\color{blue}T : That's correct. (+)} \\
\cline{1-1}\cline{3-3}
\end{tabular}
\end{center}
\caption[Sample dialogues for {\it Knowledge Acquisition} tasks]{Different tasks for {\it Knowledge Acquisition}. Crossed lines correspond to entries of the KB whose retrieval is missed.}
\end{figure*}


\subsection{Knowledge Acquisition} \label{sec:knolaq}
For the tasks in this subsection, the bot has an
incomplete KB and there are entities important to the dialogue missing from it,
see Figure \ref{MissE}.
For example, given the question  ``Which movie did Tom Hanks star in?",
the missing part could either be the entity that the teacher is asking about
(question entity for short, which is \underline{Tom Hanks} in this example),
the relation entity (\underline{starred\_actors}), the answer to the question (\underline{Forrest Gump}),
or the combination of the three.
In all cases, the bot has little chance of giving the correct answer due to the missing knowledge.
It needs to ask the teacher the answer to acquire the missing knowledge.
The teacher will give the answer and then move on to other questions (captured in the conversational history). They later will come back to reask the question.
At this point, the bot needs to give an answer since the entity is not new any more.

Though the correct answer has effectively been included in the earlier part of the dialogue as the answer to the bot's question, as we will show later, many of the tasks are not as trivial as they look when the teacher reasks the question. This is because the bot's model needs to memorize the missing entity and then construct the links between the missing entities and known ones. This is akin to the real world case where a student might make the same mistake again and again even though each time the teacher corrects them if their answer is wrong. We now detail each task in turn.

{\bf Missing Question Entity}:    The entity
that
the teacher is asking about is missing from the knowledge base.
All KB facts containing the question entity will be hidden from the bot. In the example for {\it Task 5} in Figure \ref{MissE}, since the teacher's question contains the entity \underline{Tom Hanks}, the KB facts that contain \underline{Tom Hanks} are  hidden from the bot.

{\bf Missing Answer Entity}:
The answer entity to the question is unknown to the bot.
All KB facts that contain the answer entity will be hidden. Hence, in {\it Task 6} of Figure \ref{MissE}, all KB facts containing the answer entity \underline{Forrest Gump} will be hidden from the bot.

{\bf Missing Relation Entity}:
The relation type is unknown to the bot.
In {\it Task 7} of Figure \ref{MissE},
all KB facts that express the relation \underline{starred actors} are hidden from the bot.

{\bf Missing Triples}:
The triple that expresses the relation between the question entity and the answer entity is hidden from the bot. In {\it Task 8} of Figure \ref{MissE}, the triple ``\underline{Forrest Gump} (question entity)
starred\textunderscore actors  \underline{Tom Hanks} (answer entity)" will be hidden.

{\bf Missing Everything}: The question entity, the relation entity, the answer entity are
all missing from the KB. All KB facts in
{\it Task 9} of Figure \ref{MissE} will be removed since they either contain the relation entity (i.e., starred\textunderscore actors), the question entity (i.e., \underline{Forrest Gump}) or the answer entity \underline{Tom Hanks}.

\section{Train/Test Regime}
\label{Train_Test}
We now discuss in detail the regimes we used to train and test our models,
which are divided between evaluation within our simulator and using
real  data collected via Mechanical Turk.

\if 0
three regimes we used to train and
test our models. We first wanted to validate the usefulness of asking questions 
in all the settings described in Section \ref{sec:tasks}.
Second, we wanted to assess the ability of our student bot 
to learn {\em when} to ask questions.
In order to accomplish these two
objectives we explored training our models with our simulator using two methodologies,
namely, Offline Supervised Learning and Online Reinforcement Learning.
Finally, our third objective was to validate the findings from our simulator
on real data using Mechanical Turk.
\fi 

\subsection{Simulator}

% !TEX root = AQ.tex
\label{sec:train-test}

Using our simulator, our objective was twofold.
We first wanted to validate the usefulness of asking questions 
in all the settings described in the previous section.
Second, we wanted to assess the ability of our student bot 
to learn {\em when} to ask questions.
In order to accomplish these two
objectives we explored training our models with our simulator using two methodologies,
namely, Offline Supervised Learning and Online Reinforcement Learning.


\subsubsection{Dialogue Simulator}
Here we  detail 
the simulator and 
the datasets we generated in order to realize the various scenarios 
discussed in Section~\ref{sec:tasks}. 
We focused on the problem of movieQA where we adapted the 
WikiMovies dataset proposed in \cite{weston2015towards}. The 
dataset consists of roughly $100$k questions with over $75$k entities 
from the open movie dataset (OMDb). 

Each dialogue generated by the simulator takes place between 
a student and a teacher.
The simulator samples a random question from the 
WikiMovies dataset and fetches the set of all KB facts
relevant to the chosen question. This question is assumed 
to be the one the teacher asks its student, and is referred to as 
the ``original" question. The student is first presented with the 
relevant KB facts followed by the original question. Providing the 
KB facts to the student allows us to control the exact knowledge 
the student is given access to while answering the questions. 
At this point, depending on the 
task at hand and the student's ability to answer, the 
student might choose to directly answer it or ask a ``followup"
question. The nature of the followup question will depend on 
the scenario under consideration. If the student 
answers the question, it gets a response from the teacher about 
its correctness and the conversation ends. However if the student 
poses a followup question, the teacher gives an appropriate 
response, which should give additional information to the student 
to answer the original question. In order to make things more 
complicated, the simulator pads the 
conversation with several unrelated student-teacher 
question-answer pairs. These question-answer pairs can be 
viewed as distractions and are 
used to test the student's ability to remember the additional 
knowledge provided by the teacher after it was queried. 
For each dialogue, the simulator incorporates $5$ such pairs 
($10$ sentences). We refer to these pairs as \emph{conversational 
histories}. 

For the \emph{QA} setting (see Section~\ref{sec:tasks}), the 
dialogues generated by the simulator are such that the student 
never asks a clarification question. Instead, it simply responds 
to the original question, even if it is wrong. For the dialogs in 
the \emph{AQ} setting, the student {\em always} asks a clarification 
question. The nature of the question asked is dependent on 
the scenario (whether it is 
\emph{Question Clarification}, 
\emph{Knowledge Operation}, or 
\emph{Knowledge Acquisition}) under consideration. 
In order to simulate the 
case where the student sometimes choses to directly answer 
the original question and at other times choses to ask question, 
we created training datasets, which were a combination of 
\emph{QA} and \emph{AQ} (called ``Mixed''). For all these cases, the student 
needs to give an answer to the teacher's original question at 
the end.



\subsubsection{Offline Supervised Learning} \label{sec:offline}
The motivation behind training our student models in an offline supervised
setting was primarily to test the usefulness of the ability to ask questions.
The dialogues are generated as described in the previous section,
and the bot's role is generated with a fixed policy.
We chose a policy where answers to the teacher's questions are
correct answers 50\% of the time, and incorrect
otherwise, to add a degree of realism.
Similarly, in tasks where questions can be irrelevant
%(such as in the {\it Question Verification} tasks and the {\it Knowledge Verification} tasks)
they are only asked correctly 50\% of the time.\footnote{This only makes sense
in tasks like Question or Knowledge Verification. In tasks where the question is
static such as `What do you mean?'' there is no way to ask an irrelevant question,
and we do not use this policy.}

The offline setting  explores different combinations of training and testing
scenarios, which mimic different situations in the real world.
The aim is to understand when and how observing interactions between two agents
can help the bot improve its performance for different tasks. As a result
we construct training and test sets in three ways across all tasks,
resulting in $9$ different scenarios per task, each of which correspond to
a real world scenario.

The three training sets we generated are referred to as
{\bf TrainQA, TrainAQ, and TrainMix}. TrainQA
 follows the QA setting discussed in the previous section:
the bot never asks questions and only tries to immediately answer.
TrainAQ follows the AQ setting: the student, before answering,
first always asks a question in response to the teacher's original question.
TrainMix is a combination of the two
where $50\%$ of time the student asks a question and $50\%$ does not.



The three test sets we generated are referred to as {\bf TestQA, TestAQ,
and TestModelAQ}. TestQA and TestAQ are generated similarly to TrainQA and TrainAQ,
but using a perfect fixed policy (rather than 50\% correct) for evaluation purposes.
%The setting of the TestModelAQ is different from the rest. 
In the TestModelAQ setting the model has to get the form of the question correct as well. 
In the  {\em Question Verification} and {\em Knowledge Verification} tasks there are many possible ways of forming the question and some of them are correct -- the model has to choose the right question to ask. E.\ g.\ it should ask ``Does it have something to do with the fact that Larry Crowne directed by Tom Hanks?''rather than ``Does it have something to do with the fact that Forrest Gump directed by Robert Zemeckis?'' when the latter is irrelevant (the candidate list of questions is generated from the known knowledge base entries with respect to that question).
The policy is trained
using either the TrainAQ or TrainMix set, depending on the training
scenario. The teacher will reply to the question, giving positive feedback
if the student's question is correct and no response and negative feedback otherwise. The
student will then give the final answer.
The difference between TestModelAQ and TestAQ only exists in
the {\it Question Verification} and {\it Knowledge Verification} tasks;
in other tasks there is only one way to ask the question
 and TestModelAQ and TestAQ are identical.

To summarize, for every task listed in Section~\ref{sec:tasks} we train
one model for each of the three training sets (TrainQA, TrainAQ, TrainMix)
and test each of these models on the three test sets (TestQA, TestAQ, and
TestModelAQ), resulting in $9$ combinations.
For the purpose of notation the train/test combination is denoted by
``TrainSetting+TestSetting". For example, {\it TrainAQ+TestQA} denotes
a model which is trained using the TrainAQ dataset and tested on TestQA
dataset. Each combination has a real world interpretation.  For instance,
{\it TrainAQ+TestQA} would refer to a scenario where a student can ask
the teacher questions during learning but cannot to do so while taking an
exam. Similarly, {\it TrainQA+TestQA} describes a stoic teacher that
never answers a student's question at either learning or examination time.
The setting {\it TrainQA+TestAQ} corresponds to the case where a lazy
student never asks question at learning time but gets anxious during the
examination and always asks a question.

\subsubsection{Online Reinforcement Learning (RL)}
We also explored scenarios where the student
learns the ability to decide when to ask a question. In other
words, the student learns how to learn.

Although it is in the interest of the student to ask questions at every
step of the conversation, since the response to its question will contain
extra information, we don't want our model to learn this behavior.
Each time a human student asks a question, there's a cost associated with that
action. This cost is a reflection of the patience of the teacher,
or more generally of the users interacting with the bot in the wild:
users won't find the bot engaging if it always
asks clarification questions. The student should thus be judicious about
asking questions and learn when and what to ask.
For instance, if the student is confident about the answer, there is no need
for it to ask. Or, if the teacher's question is so hard
%that the student has no clue and will likely not benefit even if it asked for clarification
that clarification is unlikely to help enough to get the answer right,
then it should also refrain from asking.

We now discuss how we model this problem under the Reinforcement Learning
framework. The bot is presented with KB facts (some facts might be missing
depending on the task) and a question. It needs to decide whether to ask a
question or not at this point. The decision  whether to ask
is made by a binary policy $P_{RLQuestion}$. If
the student  chooses to ask a question, it will be penalized by $cost_{AQ}$.
We explored different values of $cost_{AQ}$ ranging from $[0, 2]$, which we consider
as modeling the patience of the teacher.
% at the interval of 0.05.
%The values of $cost_{AQ}$ can be considered as the patience of a teacher/user.
The goal of this setting is to find the best policy for asking/not-asking questions
which would lead to the highest cumulative reward.
The teacher will appropriately reply if the student asks a question.
The student will eventually give an answer to the teacher's initial question
at the end using the policy $P_{RLAnswer}$, regardless of whether it had asked a
question. The student will get a reward of $+1$ if its final answer
is correct and  $-1$ otherwise. %\footnote{The reward values are chosen for illustration purposes only. Readers can tailor them to different situations.}.
Note that the student can ask at most one question and that the type of
question is always specified by the task under consideration.
The final reward the student gets is the cumulative reward over the
current dialogue episode. In particular the reward structure we propose is the
following:

\begin{table}[!ht]
\small
\begin{tabular}{rcc}
&Asking Question &Not asking Question \\
Final Answer Correct & 1-$cost_{AQ}$ & 1\\
Final Answer Incorrect & -1-$cost_{AQ}$ & -1\\
\end{tabular}
\caption{Reward structure for the Reinforcement Learning setting.}
\label{reward}
\end{table}

\begin{comment}
Here we mainly focus on the setting
where
 $P_{RL}(answer)$ is taken from the pre-trained models from the supervised setting and is kept fixed during RL training.
This is comparable to a real world case where the bot has already been well trained and its goal is to figure out how to tailor its question-asking policy to
the teacher's temper (capturing the cost of asking questions $cost_{AQ}$).
 The reason why we choose not to focus on
 its alternative which trains the two
 policies jointly (denoted as {\it joint training}) is because for {\it joint training}, the model will mainly focus on $P_{RL}(answer)$ i.e., guessing the correct answer from
 the dialogue history at the early stage of training. This will make many of $P_{RL}(Question)$  explorations wasted at the early training stage. In a real-world setting, we want to ask the users questions as infrequently as possible. A pre-trained $P_{RL}(answer)$ can serve such a purpose better than the joint training
 strategy.
 For reference, we  implemented the {\it joint training} as well and found that the results are similar to the fixed-$P_{RL}(Question)$ setting but with significantly longer training time.
\end{comment}
 \begin{comment}
and we do not update the latter one in the RL setting.\footnote{Such a strategy corresponds to the scenario where the bot has already been sufficiently trained or the student has already acquired sufficient knowledge. The goal here is to decide whether or not to ask question given the student/bot current ability. An alternative is to jointly train the question-asking model and answer-predicting model from the reward. } 
\end{comment}


\noindent For each of the tasks described in Section~\ref{sec:tasks}, we consider
three different RL scenarios.\\
{\bf  Good-Student}: The student will be presented with
all relevant KB facts. There are no misspellings or unknown words in
the teacher's question. This represents a knowledgable student in the real
world that knows as much as it needs to know (e.g., a large knowledge
base, large vocabulary). This setting is identical across all missing entity
tasks (5 - 9). \\
{\bf  Poor-Student}: The KB facts or the questions presented to the
student are flawed depending on each task. For example,
for the {\it Question Clarification} tasks, the student does not understand the
question due to spelling mistakes. For the  {\it Missing Question Entity}
task the entity that the teacher asks about is unknown by the student and
all facts containing the entity will be hidden from the student.  This setting is
similar to a student that is underprepared for the tasks. \\
{\bf Medium-Student}: The combination of the previous two settings where
for $50\%$ of the questions, the student has access to the full KB and there
are no new words or phrases or entities in the question,
and $50\%$ of the time the question and KB are taken from the
{\it Poor-Student} setting.


%An illustration of the  {\it poor-student} setting for Task 1
%({\it Question Paraphase}) is shown in Figure \ref{RL_illustration}.


\vspace{-3mm}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=4.5in]{img/RL.pdf}
    \vspace{-8mm}
\caption{An illustration of the {\it poor-student } setting for RL Task 1 (Question Paraphrase). }
\label{RL_illustration}
\end{figure*}



\subsection{Mechanical Turk Data} \label{sec:mturk}

Finally, to validate our approach beyond our simulator by using real language, we collected
data via Amazon Mechanical Turk.
Due to the cost of data collection, we focused on real language versions of
Tasks 4 (Knowledge Verification) and 8 (Missing Triple), 
see Secs. \ref{sec:knolop} and \ref{sec:knolaq} for the simulator versions.
That is, we collect dialogues and use them in an offline supervised learning setup
similar to Section \ref{sec:offline}. This setup allows easily reproducible experiments.

For Mechanical Turk Task 4, the bot is asked a question by a human teacher,
but before answering can ask the human if the question is related
to one of the facts it knows about from its memory. It is then required to answer the original question,
 after some additional dialog turns relating to other question/answer pairs
(called ``conversational history'', as before).
For Task 8, the bot is asked a question by a human but lacks the triple in its memory that
would be needed to answer it. 
It is allowed to ask for the missing information, the human responds to the question in free-form language.
The bot is then required to answer the original question,
 again after some ``conversational history'' has transpired.

We collect around 10,000 episodes (dialogues) for training, 1000 for validation, 
and 2500 for testing for each of the two tasks. In each case, we give
 instructions to the Turkers that still follow the original form of the task, 
but make the tasks contain realistic language written by humans.

For both tasks, while the human turkers replace the simulator that the bot was previously conversing with,
the bot's dialogue actions (capabilities) are essentially unchanged from before. 
That is, when answering questions, now the bot is required to answer a human's questions
rather than templated questions from the simulator.
When the bot is asking questions,
the bot still asks in the same form as before, e.g., questions like
``Does it have something to do with X'' for Task 4
or ``I don't know. What's the answer?'' for Task 8. However,  now its questions are answered
by a human. In both cases (asking and answering) the human data is richer
with potentially more complex language and lexical variability. Examples
of the collected dialogues are given in Figure \ref{example-MT-Tasks}.


\begin{figure*}[!ht]
\centering
\footnotesize
\begin{tabular}{|l|}
\cline{1-1}
 {\bf MTurk Task 4:  Knowledge Verification (AQ): } \\
~~kb:  In Their Skin written\_by Joshua Close, Jeremy Power Regimbal      \\
~~kb:  In Their Skin starred\_actors Selma Blair, James D'arcy, Rachel Miner, Joshua Close \\
~~{\color{blue}T: What film was written by Joshua Close?}                                           \\
~~{\color{red}S: Does it have something to do with ``In Their Skin starred\_actors Joshua Close''?}  \\
~~{\color{blue}T: No we were looking for who wrote the film not who starred in it.}                \\
~~\dots\\
%~~{\color{blue}T}/{\color{red}S }:  {Conversation History} \\
%~~{\color{blue}T: What film was written by Joshua Close?}                                           \\
%~~{\color{red}S: In Their Skin}                                          \\
\cline{1-1}
\cline{1-1}
{\bf MTurk Task 8: Missing Triple (AQ):} \\
~~{\color{blue}T :What movies does Timothy Scott Bogart have writing credit for?}\\
~~{\color{red}S:  I don't know. What's the answer?} \\
~~{\color{blue}T: Extreme Ops or The I Inside is the right answer.} \\
~~\dots\\
\cline{1-1}
\end{tabular}
\caption[Sample dialogues for Mechanical Turk versions of Tasks 4 and 8]{Sample dialogues for Mechanical Turk versions of Tasks 4 and 8. Compared to the original
tasks (see Figs \ref{AskV} and \ref{MissE}) the teacher's questions, and the teacher
 responses to the student's questions,
 are written by humans and are more complex and contain more variety.
\label{example-MT-Tasks}
}
\end{figure*}

\section{Models}
%{\color{yellow} JIWEI: GENERAL COMMENT - THIS WHOLE SECTION IS TOO VAGUE. WE NEED TO GIVE MORE DETAILS ABOUT THE MODELS AND HOW THEY
%ARE EXACTLY TRAINED.}
For both  offline supervised and online RL settings, we use the {\it End-to-End Memory Network}
model (MemN2N) \citep{sukhbaatar2015end} as a backbone. The
model takes as input the last utterance of the dialogue history (the question
from the teacher) as well as a set of memory contexts including
short-term memories (the dialogue history between the bot and the teacher) and
long-term memories (the knowledge base facts that the bot has access to),
and outputs a label.



{\bf Offline Supervised Settings}:
%Standard MemN2N models can be applied in this setting.
%To decide the final answer,
%standard MemN2N models can be straightforwardly applied.
The first learning strategy we adopt is
the reward-based imitation strategy (denoted {\it vanilla-MemN2N}) described in \newcite{weston2016dialog}, where
at training time, the model maximizes the log likelihood probability
of the correct answers the student gave (examples with incorrect final answers are discarded).
Candidate answers are words that appear in the memories,
which means the bot can only predict the entities that it has seen or known before.

We also use a variation of MemN2N called ``context MemN2N'' ({\it Cont-MemN2N} for short)
where we replace each word's embedding with the average of its embedding (random for
unseen words) and the embeddings of the other words that appear around it.
%where each word embedding is the average of the embedding associated to that particular word
%(if that word has never seen before, this is a random vector) and the embeddings of the words in its context.
We use both the preceeding and following words as context and the number of context words is
a hyperparameter selected on the dev set.

An issue with both {\it vanilla-MemN2N} and {\it Cont-MemN2N}  is that the model only makes use of
the bot's answers as signals and ignores the teacher's feedback.
We thus propose to use a  model that jointly predicts the bot's answers and the teacher's feedback
(denoted as {\it TrainQA (+FP)}).
The bot's answers are predicted using a {\it vanilla-MemN2N}  and the teacher's feedback is
predicted using the {\it Forward Prediction} (FP) model as described in \newcite{weston2016dialog}.
At training time, the models learn to jointly predict the teacher's feedback
and the answers with positive reward. At test time, the model will only predict the bot's answer.

\begin{comment}
One problem with the vanilla-MemN2N model is that for  tasks like {\it Missing Answer Entity} or {\it Missing Everything}, the model never saw the answer entity before the dialogue takes place.
At test time,
even though the answer is provided by the teacher in response to the student's question,
the bot is still hard to predict the answer correctly since the
the embedding of answer entity is still the original randomly-initialized one.
\end{comment}

For the
{\it TestModelAQ} setting described in Section \ref{Train_Test}, the model
 needs to decide the question to ask.
%, i..e, to ask
%whether the teacher's question corresponds to
%another question without spelling mistakes in Task 2 ({\em Question Verification})
%or whether the answer is related to one of the knowledge entries
%in Task 4 ({\em Knowledge Verification}).
Again, we use vanilla-MemN2N that takes as input the question and contexts,
and outputs the question the bot will ask.
%, which is
%a question without spelling mistakes in Task 2 and
%a relevant knowledge from the memory in Task 4.

{\bf Online RL Settings}:
%For the RL setting, the bot is given the question and the KB facts, part of which are hidden for tasks 3-8 for the {\it poor-student} setting.
A binary vanilla-MemN2N (denoted as $P_{RL}(Question)$) is used to decide whether
the bot should or should not ask a question, with the teacher replying if the bot does ask something.
A second MemN2N
is then used to decide the bot's answer, % given the dialogue history,
denoted as $P_{RL}(Answer)$.
$P_{RL}(Answer)$ for $QA$ and $AQ$ are two separate models, which means the bot will use different  models
for final-answer prediction
depending on whether it
 chooses to ask a question or not.\footnote{An alternative is to train one single model
for final answer prediction in both {\it AQ} and {\it QA} cases, similar to the {\it TrainMix} setting in the supervised learning setting. But we find training {\it AQ} and {\it QA} separately for the final answer prediction yields a little better result than the single model setting.}


We use the REINFORCE algorithm \citep{williams1992simple} to update $P_{RL}(Question)$ and  $P_{RL}(Answer)$.
For each dialogue,
the bot takes two sequential actions $(a_1,a_2)$:
to ask or not to ask a question (denoted as $a_1$); and guessing the final answer (denoted as $a_2$).
Let $r(a_1,a_2)$ denote the cumulative reward for the dialogue episode, computed using Table \ref{reward}.
The gradient to update the policy is given by:
\begin{equation}
\begin{aligned}
&p(a_1,a_2)=P_{RL}(Question)(a_1) \cdot  P_{RL}(answer)(a_2)\\
&\nabla J(\theta)\approx \nabla\log p(a_1,a_2) [r(a_1,a_2)-b]
\end{aligned}
\end{equation}
where $b$ is the baseline value, which is estimated using another MemN2N model
that takes as input the query $x$ and memory $C$, and outputs a scalar $b$ denoting the estimation of the future reward.
The baseline model is trained by minimizing the mean squared loss between the estimated reward $b$
and actual cumulative reward $r$, $||r-b||^2$.
We refer the readers to \newcite{zaremba2015reinforcement} for more details.
The baseline estimator model is independent from the policy models and the
error is not backpropagated back to them.

In practice, we find the following training strategy yields better results:
first train only $P_{RL}(answer)$, updating gradients only for the policy that
predicts the final answer.
After the bot's final-answer policy is sufficiently learned, train both policies in parallel.\footnote{
We implement this by running 16 epochs in total, updating only the model's policy for final answers in the first 8 epochs while updating both policies during the second 8 epochs. We pick the model that achieves the best reward on the dev set during the final 8 epochs. Due to relatively large variance for RL models, we repeat each task 5 times and keep the best model on each task.
}
This has a real-world analogy where the bot first learns the basics of the task, and then learns to
improve its performance via a question-asking policy tailored to the user's patience
(represented by $cost_{AQ}$) and its own ability to asnwer questions.

\section{Experiments}
\subsection{Offline Results}
Offline results are presented in Tables \ref{HeyHeyOOVResult}, \ref{OfflineResult}, and \ref{TestModelAQ}.
Table \ref{OfflineResult} presents results for the {\it vanilla-MemN2N} and
{\it Forward Prediction} models. Table \ref{HeyHeyOOVResult} presents results for {\it Cont-MemN2N},
which is better at handling unknown words.
We repeat each experiment 10 times and report the best result. 
Finally, Table \ref{TestModelAQ} presents results for the test scenario
 where the bot itself chooses when to ask questions.
Observations can be summarized as as follows:

\begin{table*}[t!]
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{Question Clarification }&\multicolumn{4}{c}{Knowledge Operation} \\\cmidrule(r){2-5}\cmidrule(r){6-9}
&\multicolumn{2}{c}{Task 1: Q. Paraphrase}&\multicolumn{2}{c}{Task 2: Q. Verification}&\multicolumn{2}{c}{Task 3: Ask For Relevant K.}&\multicolumn{2}{c}{Task 4: K. Verification}  \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}\cmidrule(r){8-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\\midrule
TrainQA (Context) &0.754 &0.726 & 0.742&0.684&  0.883&0.947 &  0.888&0.959\\
TrainAQ (Context)&0.640&0.889&0.643&0.807&0.716&0.985 &0.852&0.987 \\
TrainMix (Context)&0.751&0.846&0.740&0.789&0.870&0.985 & 0.875&0.985 \\\bottomrule
\end{tabular}
\hspace{0.5cm}
\begin{tabular}{ccccccccccc}\toprule
&\multicolumn{8}{c}{Knowledge Acquisition }   \\\cmidrule(r){2-11}
&\multicolumn{2}{c}{Task 5: Q. Entity}&\multicolumn{2}{c}{Task 6: Answer Entity}   &\multicolumn{2}{c}{Task 7: Relation Entity}&\multicolumn{2}{c}{Task 8: Triple}&\multicolumn{2}{c}{Task 9: Everything}   \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}\cmidrule(r){8-9}\cmidrule(r){10-11}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ&TestQA&TestAQ \\\midrule
TrainQA (Context)&$<$0.01 &0.224&$<$0.01&0.120&0.241&0.301& 0.339 &0.251 & $<$0.01&0.058\\
TrainAQ (Context)& $<$0.01 &0.639& $<$0.01&0.885&0.143&0.893&0.154&0.884 &  $<$0.01&0.908\\
TrainMix (Context)& $<$0.01 &0.632&$<$0.01& 0.852 &0.216&0.898&0.298&0.886&  $<$0.01&0.903\\\bottomrule
\end{tabular}
\caption{Results for Cont-MemN2N on different tasks.}
\label{HeyHeyOOVResult}
\end{table*}

\begin{table*}[h]
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{Question Clarification }&\multicolumn{4}{c}{Knowledge Operation} \\\cmidrule(r){2-5}\cmidrule(r){6-9}
&\multicolumn{2}{c}{Task 1: Q. Paraphrase}&\multicolumn{2}{c}{Task 2: Q. Verification}&\multicolumn{2}{c}{Task 3: Ask For Relevant K.}&\multicolumn{2}{c}{Task 4: K. Verification}  \\\cmidrule(r){2-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\\midrule
TrainQA &0.338 &0.284 &0.340&0.271&  0.462&0.344 &  0.482&0.322\\
TrainAQ&0.213&0.450&0.225&0.373&0.187&0.632  &0.283&0.540 \\
TrainAQ(+FP)&0.288&0.464&0.146&0.320&0.342&0.631&0.311&0.524 \\
TrainMix&0.326&0.373&0.329&0.326&0.442&0.558 & 0.476&0.491 \\\bottomrule
\end{tabular}
\centering
\scriptsize
\begin{tabular}{ccccccccccc}\toprule
&\multicolumn{8}{c}{Knowledge Acquisition }   \\\cmidrule(r){2-11}
&\multicolumn{2}{c}{Task 5:  Q. Entity}&\multicolumn{2}{c}{Task 6: Answer Entity}   &\multicolumn{2}{c}{Task 7: Relation Entity}&\multicolumn{2}{c}{Task 8: Triple}&\multicolumn{2}{c}{Task 9: Everything}   \\\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}\cmidrule(r){8-9}\cmidrule(r){10-11}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ&TestQA&TestAQ \\\midrule
TrainQA (vanila)&$<0.01$ &0.223& $<$0.01&$<$0.01&0.109&0.129& 0.201&0.259 & $<$0.01&$<$0.01\\
TrainAQ (vanila)&$<0.01$ &0.660& $<$0.01&$<$0.01&0.082&0.156&0.124&0.664 & $<$0.01&$<$0.01\\
TrainAQ(+FP)&$<0.01$& 0.742&$<0.01$&$<0.01$& 0.085&0.188&0.064&0.702&$<$0.01&$<$0.01  \\
Mix (vanila)& $<$0.01 &0.630&$<$0.01& $<$0.01 &0.070&0.152&0.180&0.572& $<$0.01&$<$0.01\\\bottomrule
\end{tabular}
\caption{Results for offline settings using memory networks.}
\label{OfflineResult}
\end{table*}

%\begin{itemize}
%\item
- Asking questions helps at test time, which is intuitive since it provides additional evidence:
\begin{itemize}
\item  {\it TrainAQ+TestAQ} (questions can be asked at both training and test time) performs the best across all the settings.
\item {\it TrainQA+TestAQ} (questions can be asked at training time but not at test time) performs worse than {\it TrainQA+TestQA} (questions can be asked at neither training nor test time) in tasks {\it Question Clarification} and {\it Knowledge Operation} due to the discrepancy between training and testing.
\item   {\it TrainQA+TestAQ}  performs better than {\it TrainQA+TestQA}  on all {\it Knowledge Acquisition} tasks, the only exception being the {\it Cont-MemN2N} model on the {\it Missing Triple} setting. The explanation is that for most tasks in {\it Knowledge Acquisition}, the learner has no chance of giving the correct answer without asking questions. The benefit from asking is thus large enough to compensate for the negative effect introduced by data discrepancy between training and test time.
\item {\it TrainMix} offers flexibility in bridging the gap between datasets generated using QA and AQ, very slightly underperforming {\it TrainAQ+TestAQ}, but gives competitive results on both {\it TestQA} and {\it TestAQ} in  the {\it Question Clarification} and {\it Knowledge Operations} tasks.
\item {\it TrainAQ+TestQA} (allowing questions at training time but forbid questions at test time) performs the worst, even worse than
{\it TrainQA+TestQA}. This has a real-world analogy where a student becomes dependent on the teacher answering their questions, later struggling to answer the test questions without help.
\item In the {\it Missing Question Entity} task (the student does not know about the question entity), the {\it Missing Answer Entity} task (the student does not know about the answer entity), and {\it Missing Everything} task, the bot achieves accuracy less than 0.01  if
not asking questions at test time (i.e., {\it TestQA}).
\item The performance of {\it TestModelAQ}, where the bot relies on its model to ask questions at test time (and thus can ask irrelevant questions) performs similarly to asking the correct question at test time ({\it TestAQ}) and better than not asking questions ({\it TestQA}).
\end{itemize}
- {\it Cont-MemN2N} significantly outperforms {\it vanilla-MemN2N}. One explanation
is that considering context provides significant evidence distinguishing correct
answers from candidates in the dialogue history, %(neighbors of words in dialogue
%history are very different from neighbors of entities in the knowledge base).
especially in cases where the model encounters unfamiliar words.
%\end{itemize}




\begin{table}[h!]
\centering
\small
\begin{tabular}{ccc}\toprule
& Question Clarification & Knowledge Acquisition \\
&Task 2: Q. Verification& Task 4: K. Verification\\
&TestModelAQ&TestModelAQ\\\midrule
TrainAQ&0.382 &0.480 \\
TrainAQ(+FP)&0.344&0.501 \\
TrainMix&0.352&0.469 \\\bottomrule
\end{tabular}
\caption{Results for TestModelAQ settings. }
\label{TestModelAQ}
\end{table}



\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=2in]{img/Task2Pro.pdf}
\includegraphics[width=2in]{img/Task2Acc.pdf}\\
\includegraphics[width=2in]{img/Task6Pro.pdf}
\includegraphics[width=2in]{img/Task6Acc.pdf}\\
\end{center}
\caption{Results of online learning for Task 2 and Task 6
\label{part-result}
}
\end{figure*}

\subsection{RL Results}
For the RL settings,
we  present  results for Task 2  ({\it Question Verification})
  and Task 6 ({\it Missing Answer Entities}) in Figure \ref{part-result}.
Task 2 represents scenarios where different types of student have different
abilities to correctly answer  questions (e.g., a poor student can still sometimes give
correct answers even when they do not fully understand the question).
Task 6 represents tasks where a poor learner who lacks the knowledge necessary
to answer the question can hardly give a correct answer.
All types of students including the good student will theoretically benefit from asking questions
(asking for the correct answer) in Task 6.
    We show the percentage of question-asking versus the cost of AQ on the test set
and  the accuracy of question-answering on the test set
vs the cost of AQ. Our main findings were: %Observations are summarized as follows:
\begin{itemize}
\item
A good student does not need to ask questions
in Task 2  ({\it Question Verification}),
because they already understand the question. The student will raise questions
asking for the correct answer when cost is low for Task 6 ({\it Missing Answer Entities}).
\item A poor student always asks questions when the cost is low.
 As the cost increases, the frequency of question-asking declines.
\item As the AQ cost increases gradually, good students will stop asking questions
earlier than the medium and poor students. The explanation is intuitive:
poor students benefit more from asking questions than good students, so they
continue asking even with higher penalties.
\item As the probability of question-asking declines, the accuracy for poor and
medium students drops.  Good students are more resilient to not asking questions.
\end{itemize}

\subsection{Mechanical Turk Results} \label{sec:res-mturker}
Results for the Mechanical Turk Tasks are given in Table \ref{res:mturk-mainy}.
We again compare vanilla-MemN2N and Cont-MemN2N, using the same
 TrainAQ/TrainQA and TestAQ/TestQA combinations as before, 
for Tasks 4 and 8 as described in  Section \ref{sec:mturk}.
We tune hyperparameters on the validation set and 
repeat each experiment 10 times and report the best result. 

While performance is lower than on the related Task 4 and Task 8 simulator tasks,
we still arrive at the same trends and conclusions when real data from humans is used.
The performance was expected to be lower because (i) real data has more lexical variety, complexity,
 and noise; and (ii) the training set was smaller due to data collection costs (10k vs. 180k).

More importantly, the same main conclusion is observed as before:
TrainAQ+TestAQ (questions can be asked at both training and test time) 
performs the best across all the settings. That is, we show that a bot asking questions to 
humans learns to outperform one that only answers them.


We also provide additional experiments.
In Table \ref{res:mturk-mainy}, results were shown when training and testing on the collected
Mechanical Turk data (around 10,000 episodes of training dialogues for training).
As we collected the data in the same settings as Task 4 and 8
of our simulator, we could also consider supplementing training with simulated 
data as well, of which we have a larger amount (over 100,000 episodes).
Note this is {\em only for training}, we will 
still test on the real (Mechanical Turk collected) data.
Although the simulated data has less lexical variety as it is built from templates, the 
larger size might obtain improve results. 

Results are given in Table \ref{res:mturk_main2} when training on the combination of real and simulator data,
and testing on real data. This should be compared to 
training on only the real data (Table \ref{res:mturk_main}) and only on the simulator
data (Table \ref{res:mturk_main3}).
The best results are obtained from the combination of simulator and real data. The best 
real data only results (selecting over algorithm and training strategy) on both tasks outperform
the best results using simulator data, i.e. 
using Cont-MemN2N with the Train AQ / TestAQ setting) 0.774 and 0.797 is obtained
 vs. 0.714 and 0.788 for Tasks 4 and 8 respectively.
This is despite there being far fewer examples of real data compared to simulator data.
Overall we obtain two main conclusions from this additional experiment:
(i) real data is indeed measurably superior to simulated data for training our models;
(ii) in all cases (across different algorithms, tasks, and data types -- be they real data, simulated data 
or combinations)
 the bot asking questions (AQ) outperforms it only answering questions and not asking them (QA).
The latter reinforces the main result of the paper.



\begin{table*}[t!]
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{vanilla-MemN2N}&\multicolumn{4}{c}{Cont-MemN2N} \\\cmidrule{2-5}\cmidrule{6-9}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}  \\\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\\midrule
TrainQA & 0.331 & 0.313  & 0.133 & 0.162 & 0.712  & 0.703  & 0.308 & 0.234\\
TrainAQ & 0.318 & 0.375  & 0.072 & 0.422 & 0.679  & 0.774  & 0.137 & 0.797 \\\bottomrule
\end{tabular}
\caption{Mechanical Turk Task Results. {\it Asking Questions} (AQ) outperforms only 
answering questions without asking (QA).
\label{res:mturk-mainy}}
\end{table*}



\begin{table*}[h]
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{vanilla-MemN2N}&\multicolumn{4}{c}{Cont-MemN2N} \\\cmidrule{2-5}\cmidrule{6-9}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}  \\\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\
TrainQA & 0.331 & 0.313  & 0.133 & 0.162 & 0.712  & 0.703  & 0.308 & 0.234\\
TrainAQ & 0.318 & 0.375  & 0.072 & 0.422 & 0.679  & 0.774  & 0.137 & 0.797 \\\bottomrule
\end{tabular}
\caption{Mechanical Turk Task Results, using real data for training and testing.
\label{res:mturk_main}}
%\end{table*}
%\begin{table*}[h]
\vspace{1cm}
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{vanilla-MemN2N}&\multicolumn{4}{c}{Cont-MemN2N} \\\cmidrule{2-5}\cmidrule{6-9}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}  \\\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\
TrainQA & 0.356 & 0.311  & 0.128 & 0.174 & 0.733  & 0.717  & 0.368 & 0.352\\
TrainAQ & 0.340 & 0.445  & 0.150 & 0.487 & 0.704  & 0.792  & 0.251 & 0.825 \\\bottomrule
\end{tabular}
\caption{Results on Mechanical Turk Tasks using a combination of real and simulated data for training,
testing on real data.
\label{res:mturk_main2}}
%\end{table*}
%\begin{table*}[h]
\vspace{1cm}
\centering
\scriptsize
\begin{tabular}{ccccccccc}\toprule
&\multicolumn{4}{c}{vanilla-MemN2N}&\multicolumn{4}{c}{Cont-MemN2N} \\\cmidrule{2-5}\cmidrule{6-9}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}
&\multicolumn{2}{c}{Task 4: K. Verification}&\multicolumn{2}{c}{Task 8: Triple}  \\\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
Train \textbackslash Test &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ &TestQA&TestAQ\\
TrainQA & 0.340 & 0.311  & 0.120 & 0.165 & 0.665  & 0.648  & 0.349 & 0.342\\
TrainAQ & 0.326 & 0.390  & 0.067 & 0.405 & 0.642  & 0.714  & 0.197 & 0.788 \\\bottomrule
\end{tabular}
\caption{Results on Mechanical Turk Tasks using only simulated data for training,
but testing on real data.
\label{res:mturk_main3}}
\end{table*}






